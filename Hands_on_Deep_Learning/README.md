# Deep-Learning - This repository contains codes to train deep learning frameworks for different applications.

## Deep Learning using Fastai
| # | **File name** |  **Description** |
| ---------- |--------- | ------------------------------------------------| 
|1|[Pet_breed_classification](https://github.com/ruchikaverma-iitg/ML-DL-RL_Codes/blob/master/Hands_on_Deep_Learning/Deep_Learning_using_Fastai/Pet_breed_classification.ipynb)|Code to train Resnet-34 and Resnet-50 models for cat vs. dog breed classification problem. This code includes 1-cycle learning rate adaptation policy and did classification on 37 breeds.|
|2|[MNIST_classification](https://github.com/ruchikaverma-iitg/ML-DL-RL_Codes/blob/master/Hands_on_Deep_Learning/Deep_Learning_using_Fastai/MNIST_classification.ipynb)|Code to train Resnet-18 model for MNIST digit classification.|
|3|[Download_and_classify_data_from_web](https://github.com/ruchikaverma-iitg/ML-DL-RL_Codes/blob/master/Hands_on_Deep_Learning/Deep_Learning_using_Fastai/Download_and%20classify_data_from_web.ipynb)|Code to download (from web) and classify (using deep neural network) images of 3 sweets. Also it demonstrates how to clean up the data by visualizing misclassified images.|
|4|[SGD_linear_regression](https://github.com/ruchikaverma-iitg/ML-DL-RL_Codes/blob/master/Hands_on_Deep_Learning/Deep_Learning_using_Fastai/SGD_linear_regression.ipynb)|Stochastic gradient descent on regression problem.|
|5|[Multi-label prediction](https://github.com/ruchikaverma-iitg/ML-DL-RL_Codes/blob/master/Hands_on_Deep_Learning/Deep_Learning_using_Fastai/Multi-label%20prediction.ipynb)|Multi-label prediction using Resnet-50 model on Planet Amazon dataset obtained from [Kaggle](https://www.kaggle.com/c/planet-understanding-the-amazon-from-space).|
|6|[Image_Segmentation](https://github.com/ruchikaverma-iitg/ML-DL-RL_Codes/blob/master/Hands_on_Deep_Learning/Deep_Learning_using_Fastai/Image_Segmentation.ipynb)|Image segmentation using U-Net and with Resnet-18 enncoder on Camvid dataset.|




## Ipython notebooks of [Deep Neural Networks with PyTorch](https://www.coursera.org/learn/deep-neural-networks-with-pytorch/home/welcome)
| # | **File name** |  **Description** |
| ---------- |--------- | ------------------------------------------------| 
|1|[1D_tensors](https://github.com/ruchikaverma-iitg/ML-DL-RL_Codes/blob/master/Hands_on_Deep_Learning/Deep_Neural_Networks_with_PyTorch/L1_1D_tensors.ipynb)| Basic operations on 1D tensors|
|2|[2D_tensors](https://github.com/ruchikaverma-iitg/ML-DL-RL_Codes/blob/master/Hands_on_Deep_Learning/Deep_Neural_Networks_with_PyTorch/L2_Two-Dimensional_Tensors.ipynb)| Basic operations on 2D tensors|
|3|[Derivatives](https://github.com/ruchikaverma-iitg/ML-DL-RL_Codes/blob/master/Hands_on_Deep_Learning/Deep_Neural_Networks_with_PyTorch/L3_derivativesandGraphsinPytorch.ipynb)| Derivatives in Pytorch|
|4|[Toy_dataset](https://github.com/ruchikaverma-iitg/ML-DL-RL_Codes/blob/master/Hands_on_Deep_Learning/Deep_Neural_Networks_with_PyTorch/L4_simple_data_set.ipynb)| Creating a toy dataset in Pytorch, compose and perform transformations on it|
|5|[Datasets_and_transforms](https://github.com/ruchikaverma-iitg/ML-DL-RL_Codes/blob/master/Hands_on_Deep_Learning/Deep_Neural_Networks_with_PyTorch/L5_Datasets_and_transforms.ipynb)| Build an image dataset object and perform pre-build transformations using torchvision.transforms on it|
|6|[MNIST_data_&_transforms](https://github.com/ruchikaverma-iitg/ML-DL-RL_Codes/blob/master/Hands_on_Deep_Learning/Deep_Neural_Networks_with_PyTorch/L6_pre-Built%20Datasets_and_transforms.ipynb)| How to use pre-built MNIST dataset and perform transformations on it|
|7|[Regression_prediction](https://github.com/ruchikaverma-iitg/ML-DL-RL_Codes/blob/master/Hands_on_Deep_Learning/Deep_Neural_Networks_with_PyTorch/L7_prediction_on_1D_input.ipynb)| Make predictions for multiple 1D inputs using linear class|
|8|[1D_Linear_regression_1_parameter](https://github.com/ruchikaverma-iitg/ML-DL-RL_Codes/blob/master/Hands_on_Deep_Learning/Deep_Neural_Networks_with_PyTorch/L8_linear_regression_one_parameter.ipynb)| Create linear regression model using 1 parameter, cost/criterion function using MSE, and plot parameters as well as loss values|
|9|[1D_Linear_regression_2_parameters](https://github.com/ruchikaverma-iitg/ML-DL-RL_Codes/blob/master/Hands_on_Deep_Learning/Deep_Neural_Networks_with_PyTorch/L9_training_slope_and_bias.ipynb)| 1D Linear regression model using 2 parameters (w and b). Visualize the data space and the parameter space during training via batch gradient descent|
|10|[Stochastic_gradient_descent](https://github.com/ruchikaverma-iitg/ML-DL-RL_Codes/blob/master/Hands_on_Deep_Learning/Deep_Neural_Networks_with_PyTorch/L10_stochastic_gradient_descent.ipynb)| 1D Linear regression using stochastic gradient descent|
|11|[Mini_batch_gradient_descent](https://github.com/ruchikaverma-iitg/ML-DL-RL_Codes/blob/master/Hands_on_Deep_Learning/Deep_Neural_Networks_with_PyTorch/L11_mini-batch_gradient_descent.ipynb)| 1D Linear regression using mini-batch gradient descent. This code also includes comparison between batch, stochastic and mini-batch gradient descent with different batch sizes|
|12|[Mini_batch_gradient_descent2](https://github.com/ruchikaverma-iitg/ML-DL-RL_Codes/blob/master/Hands_on_Deep_Learning/Deep_Neural_Networks_with_PyTorch/L12_PyTorchway.ipynb)| 1D Linear regression using  PyTorch build-in functions|
|13|[Models_with_different_LR](https://github.com/ruchikaverma-iitg/ML-DL-RL_Codes/blob/master/Hands_on_Deep_Learning/Deep_Neural_Networks_with_PyTorch/L13_Models_with_different_LR.ipynb)| 1D Linear regression with different learning rates and view results such as training and validation losses at different LR|
|14|[Multiple_linear_regression_prediction](https://github.com/ruchikaverma-iitg/ML-DL-RL_Codes/blob/master/Hands_on_Deep_Learning/Deep_Neural_Networks_with_PyTorch/L14_multiple_linear_regression_prediction.ipynb)| Multiple linear regression prediction (preparing forward propagation with 1xn tensor input)|
|15|[Multiple_linear_regression_training](https://github.com/ruchikaverma-iitg/ML-DL-RL_Codes/blob/master/Hands_on_Deep_Learning/Deep_Neural_Networks_with_PyTorch/L15_multiple_linear_regression_training.ipynb)| Multiple linear regression training with input of 1xn tensor|
|16|[Multi_target_linear_regression](https://github.com/ruchikaverma-iitg/ML-DL-RL_Codes/blob/master/Hands_on_Deep_Learning/Deep_Neural_Networks_with_PyTorch/L16_multi-target_linear_regression.ipynb)| Multiple target linear regression prediction (forward propagation)|
|17|[training_multiple_output_linear_regression](https://github.com/ruchikaverma-iitg/ML-DL-RL_Codes/blob/master/Hands_on_Deep_Learning/Deep_Neural_Networks_with_PyTorch/L17_training_multiple_output_linear_regression.ipynb)| pytorch build in functions to train multiple target linear regression|
|18|[logistic_regression_prediction](https://github.com/ruchikaverma-iitg/ML-DL-RL_Codes/blob/master/Hands_on_Deep_Learning/Deep_Neural_Networks_with_PyTorch/L18_logistic_regression_prediction.ipynb)| Prediction using sigmoid/logistic function|
|19|[logistic_regression_prediction](https://github.com/ruchikaverma-iitg/ML-DL-RL_Codes/blob/master/Hands_on_Deep_Learning/Deep_Neural_Networks_with_PyTorch/L19_Bad_initialization_logistic_regression_with_mean_square_error.ipynb)| Illustration of poor performance of logistic regression via bad parameters initialization|
|20|[Softmax_in_1D](https://github.com/ruchikaverma-iitg/ML-DL-RL_Codes/blob/master/Hands_on_Deep_Learning/Deep_Neural_Networks_with_PyTorch/L20_softmax_in_1D.ipynb)| Building a Softmax classifier in 1D|
|21|[predicting_MNIST_using_Softmax](https://github.com/ruchikaverma-iitg/ML-DL-RL_Codes/blob/master/Hands_on_Deep_Learning/Deep_Neural_Networks_with_PyTorch/L21_predicting_MNIST_using_Softmax.ipynb)| Classify handwritten digits from the MNIST database by using Softmax classifier and visualize parameters learned for each class following model training|
|22|[simpleNN_1hiddenlayer](https://github.com/ruchikaverma-iitg/ML-DL-RL_Codes/blob/master/Hands_on_Deep_Learning/Deep_Neural_Networks_with_PyTorch/L22_simpleNN_1hiddenlayer.ipynb)| Simple Neural Network with 1 hidden layer|
|23|[NN_more_hidden_neurons](https://github.com/ruchikaverma-iitg/ML-DL-RL_Codes/blob/master/Hands_on_Deep_Learning/Deep_Neural_Networks_with_PyTorch/L23_NN_more_hidden_neurons.ipynb)| Neural Networks with 1 hidden layer (more neurons)|
|24|[Neural_networks](https://github.com/ruchikaverma-iitg/ML-DL-RL_Codes/blob/master/Hands_on_Deep_Learning/Deep_Neural_Networks_with_PyTorch/L24_Neural_network.ipynb)| Building a neural network with 1 hidden layer to classify noisy XOR data|
|25|[1_layer_neural_network_MNIST](https://github.com/ruchikaverma-iitg/ML-DL-RL_Codes/blob/master/Hands_on_Deep_Learning/Deep_Neural_Networks_with_PyTorch/L25_1layer_neural_network_MNIST.ipynb)| Neural networks with 1 hidden layer to classify MNIST data|
|26|[Activation_function](https://github.com/ruchikaverma-iitg/ML-DL-RL_Codes/blob/master/Hands_on_Deep_Learning/Deep_Neural_Networks_with_PyTorch/L26_activation_function.ipynb)| How to apply different Activation functions in Neural Network|
|27|[Different_activations_on_neural_network](https://github.com/ruchikaverma-iitg/ML-DL-RL_Codes/blob/master/Hands_on_Deep_Learning/Deep_Neural_Networks_with_PyTorch/L27_different_activations_on_neural_network.ipynb)| Apply different activation functions in Neural Network on the MNIST dataset|
|28|[Deep_Neural_Networks](https://github.com/ruchikaverma-iitg/ML-DL-RL_Codes/blob/master/Hands_on_Deep_Learning/Deep_Neural_Networks_with_PyTorch/L28_Deep_Neural_Networks.ipynb)| Deep Neural Networks with 2 hidden layers on the MNIST dataset|
|29|[Deeper_Neural_Networks](https://github.com/ruchikaverma-iitg/ML-DL-RL_Codes/blob/master/Hands_on_Deep_Learning/Deep_Neural_Networks_with_PyTorch/L29_Deeper_Neural_Networks.ipynb)| Deep Neural Networks with 3 hidden layers using nn.ModuleList()|
|30|[Dropout_prediction](https://github.com/ruchikaverma-iitg/ML-DL-RL_Codes/blob/master/Hands_on_Deep_Learning/Deep_Neural_Networks_with_PyTorch/L30_dropout_prediction.ipynb)| Deep Neural Networks with dropout for classification|
|31|[Dropout_regression](https://github.com/ruchikaverma-iitg/ML-DL-RL_Codes/blob/master/Hands_on_Deep_Learning/Deep_Neural_Networks_with_PyTorch/L31_dropout_Regression.ipynb)| Using dropout in regression|
|32|[Weight_initialization](https://github.com/ruchikaverma-iitg/ML-DL-RL_Codes/blob/master/Hands_on_Deep_Learning/Deep_Neural_Networks_with_PyTorch/L32_weight_initialization.ipynb)| Performance of neural networks with constant (w = 1) vs. default weight initialization|
|33|[Xavier_initialization](https://github.com/ruchikaverma-iitg/ML-DL-RL_Codes/blob/master/Hands_on_Deep_Learning/Deep_Neural_Networks_with_PyTorch/L33_Xavier_initialization.ipynb)| Performance of neural networks with uniform, default, and Xavier initialization|
|34|[He_initialization](https://github.com/ruchikaverma-iitg/ML-DL-RL_Codes/blob/master/Hands_on_Deep_Learning/Deep_Neural_Networks_with_PyTorch/L34_He_Initialization.ipynb)| Performance of neural networks with uniform, default, and He initialization|
|35|[MomentumwithPolynomialFunctions](https://github.com/ruchikaverma-iitg/ML-DL-RL_Codes/blob/master/Hands_on_Deep_Learning/Deep_Neural_Networks_with_PyTorch/L35_MomentumwithPolynomialFunctions.ipynb)| Use of momentum in the model optimization|
|36|[MomentumwithPolynomialFunctions](https://github.com/ruchikaverma-iitg/ML-DL-RL_Codes/blob/master/Hands_on_Deep_Learning/Deep_Neural_Networks_with_PyTorch/L36_NeuralNetworkswithMomentum.ipynb)| Neural networks model optimization with different momentum values|
|37|[BachNorm](https://github.com/ruchikaverma-iitg/ML-DL-RL_Codes/blob/master/Hands_on_Deep_Learning/Deep_Neural_Networks_with_PyTorch/L37_BachNorm.ipynb)| Comparison of neural networks with and without batch normalization|
|38|[Convolution](https://github.com/ruchikaverma-iitg/ML-DL-RL_Codes/blob/master/Hands_on_Deep_Learning/Deep_Neural_Networks_with_PyTorch/L38_Convolution.ipynb)| Convolution on an image and estimate the output size using kernel of K size|
|39|[MaxPooling](https://github.com/ruchikaverma-iitg/ML-DL-RL_Codes/blob/master/Hands_on_Deep_Learning/Deep_Neural_Networks_with_PyTorch/L39_Activation_max_pooling.ipynb)| Application of activation function and max pooling|
|40|[Multiple_Channel_Convolution](https://github.com/ruchikaverma-iitg/ML-DL-RL_Codes/blob/master/Hands_on_Deep_Learning/Deep_Neural_Networks_with_PyTorch/L40_Multiple%20Channel%20Convolution.ipynb)| Convolutions using multiple input and output channels|
|41|[ConvolutionalNeuralNetworkexample](https://github.com/ruchikaverma-iitg/ML-DL-RL_Codes/blob/master/Hands_on_Deep_Learning/Deep_Neural_Networks_with_PyTorch/L41_ConvolutionalNeuralNetworkexample.ipynb)| Example of convolutional neural network|
|42|[CNN_Small_Image](https://github.com/ruchikaverma-iitg/ML-DL-RL_Codes/blob/master/Hands_on_Deep_Learning/Deep_Neural_Networks_with_PyTorch/L42_CNN_Small_Image.ipynb)| Build CNN using small MNIST images, visualize learned parameters and plot testing image activations after each layer. The code also plot the mis-classified samples|
|43|[CNN_Small_Image_batch](https://github.com/ruchikaverma-iitg/ML-DL-RL_Codes/blob/master/Hands_on_Deep_Learning/Deep_Neural_Networks_with_PyTorch/L43_CNN_Small_Image_batch.ipynb)| Compare a CNN using batch normalization with a regular CNN to classify handwritten digits from the MNIST database|

## Ipython notebooks of [Building Deep Learning Models with TensorFlow](https://www.coursera.org/learn/building-deep-learning-models-with-tensorflow/home/welcome)

| # | **File name** |  **Description** |
| ---------- |--------- | ------------------------------------------------| 
|1|[TF-Hello-World](https://github.com/ruchikaverma-iitg/ML-DL-RL_Codes/blob/master/Hands_on_Deep_Learning/Building_Deep_Learning_Models_with_TensorFlow/L1_TensorFlow-Hello-World.ipynb)| Basic operations on tensors using TensorFlow|
|2|[LinearRegressionwithTf](https://github.com/ruchikaverma-iitg/ML-DL-RL_Codes/blob/master/Hands_on_Deep_Learning/Building_Deep_Learning_Models_with_TensorFlow/L2-LinearRegressionwithTf.ipynb)| Linear regression with TensorFlow|
|3|[LogisticRegressionwithTf](https://github.com/ruchikaverma-iitg/ML-DL-RL_Codes/blob/master/Hands_on_Deep_Learning/Building_Deep_Learning_Models_with_TensorFlow/L3-LogisticRegressionwithTf.ipynb)| Logistic regression with TensorFlow|
|4|[CNN-MNIST-Dataset](https://github.com/ruchikaverma-iitg/ML-DL-RL_Codes/blob/master/Hands_on_Deep_Learning/Building_Deep_Learning_Models_with_TensorFlow/L4-CNN-MNIST-Dataset.ipynb)| Comparison of MLP and CNN on MNIST dataset. This code also visualizes the activation units in each convolutional layer.|
|5|[LSTM](https://github.com/ruchikaverma-iitg/ML-DL-RL_Codes/blob/master/Hands_on_Deep_Learning/Building_Deep_Learning_Models_with_TensorFlow/L5-LSTM.ipynb)| An example of Long Short-Term Memory (LSTM) model|
|6|[LSTM_LanguageModelling](https://github.com/ruchikaverma-iitg/ML-DL-RL_Codes/blob/master/Hands_on_Deep_Learning/Building_Deep_Learning_Models_with_TensorFlow/L6-LSTM-LanguageModelling_with_results.ipynb)| Application of LSTM for Language Modeling|
|7|[RBM-MNIST](https://github.com/ruchikaverma-iitg/ML-DL-RL_Codes/blob/master/Hands_on_Deep_Learning/Building_Deep_Learning_Models_with_TensorFlow/L7-RBM-MNIST.ipynb)| Restricted Boltzmann Machine (RBM) on MNIST data|


## Ipython notebooks of [Introduction to Deep Learning & Neural Networks with Keras](https://www.coursera.org/learn/introduction-to-deep-learning-with-keras)
| # | **File name** |  **Description** |
| ---------- |--------- | ------------------------------------------------| 
|1|[Forward_Propagation](https://github.com/ruchikaverma-iitg/ML-DL-RL_Codes/blob/master/Hands_on_Deep_Learning/Introduction_to_Deep_Learning_and_Neural_Networks_with_Keras/L1-Forward-Propagation.ipynb)|An example code to demonstrate how a neural network performs predictions using forward propagation|
|2|[Regression_with_Keras](https://github.com/ruchikaverma-iitg/ML-DL-RL_Codes/blob/master/Hands_on_Deep_Learning/Introduction_to_Deep_Learning_and_Neural_Networks_with_Keras/L2-Regression-with-Keras.ipynb)|Building a neural network for a regression problem|
|3|[Classification_with_Keras](https://github.com/ruchikaverma-iitg/ML-DL-RL_Codes/blob/master/Hands_on_Deep_Learning/Introduction_to_Deep_Learning_and_Neural_Networks_with_Keras/L3-Classification-with-Keras.ipynb)|Building a neural network for a classification problem using MNIST dataset|
|4|[Convolutional_Neural_Networks](https://github.com/ruchikaverma-iitg/ML-DL-RL_Codes/blob/master/Hands_on_Deep_Learning/Introduction_to_Deep_Learning_and_Neural_Networks_with_Keras/L4-Convolutional-Neural-Networks-with-Keras.ipynb)|Building a convolutional neural network with user defined convolutional and pooling layers|
