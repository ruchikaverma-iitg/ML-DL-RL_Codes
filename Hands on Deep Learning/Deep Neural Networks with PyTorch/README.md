
## Ipython notebooks of [Deep Neural Networks with PyTorch](https://www.coursera.org/learn/deep-neural-networks-with-pytorch/home/welcome)
| # | **File name** |  **Description** |
| ---------- |--------- | ------------------------------------------------| 
|1|[1D_tensors](https://github.com/ruchikaverma-iitg/ML-DL-RL_Codes/blob/master/Hands%20on%20Deep%20Learning/Deep%20Neural%20Networks%20with%20PyTorch/L1_1D_tensors.ipynb)| Basic operations on 1D tensors|
|2|[2D_tensors](https://github.com/ruchikaverma-iitg/ML-DL-RL_Codes/blob/master/Hands%20on%20Deep%20Learning/Deep%20Neural%20Networks%20with%20PyTorch/L2_Two-Dimensional_Tensors.ipynb)| Basic operations on 2D tensors|
|3|[Derivatives](https://github.com/ruchikaverma-iitg/ML-DL-RL_Codes/blob/master/Hands%20on%20Deep%20Learning/Deep%20Neural%20Networks%20with%20PyTorch/L3_derivativesandGraphsinPytorch.ipynb)| Derivatives in Pytorch|
|4|[Toy_dataset](https://github.com/ruchikaverma-iitg/ML-DL-RL_Codes/blob/master/Hands%20on%20Deep%20Learning/Deep%20Neural%20Networks%20with%20PyTorch/L4_simple_data_set.ipynb)| Creating a toy dataset in Pytorch, compose and perform transformations on it|
|5|[Datasets_and_transforms](https://github.com/ruchikaverma-iitg/ML-DL-RL_Codes/blob/master/Hands%20on%20Deep%20Learning/Deep%20Neural%20Networks%20with%20PyTorch/L5_Datasets_and_transforms.ipynb)| Build an image dataset object and perform pre-build transformations using torchvision.transforms on it|
|6|[MNIST_data_&_transforms](https://github.com/ruchikaverma-iitg/ML-DL-RL_Codes/blob/master/Hands%20on%20Deep%20Learning/Deep%20Neural%20Networks%20with%20PyTorch/L6_pre-Built%20Datasets_and_transforms.ipynb)| How to use pre-built MNIST dataset and perform transformations on it|
|7|[Regression_prediction](https://github.com/ruchikaverma-iitg/ML-DL-RL_Codes/blob/master/Hands%20on%20Deep%20Learning/Deep%20Neural%20Networks%20with%20PyTorch/L7_prediction_on_1D_input.ipynb)| Make predictions for multiple 1D inputs using linear class|
|8|[1D_Linear_regression_1_parameter](https://github.com/ruchikaverma-iitg/ML-DL-RL_Codes/blob/master/Hands%20on%20Deep%20Learning/Deep%20Neural%20Networks%20with%20PyTorch/L8_linear_regression_one_parameter.ipynb)| Create linear regression model using 1 parameter, cost/criterion function using MSE, and plot parameters as well as loss values|
|9|[1D_Linear_regression_2_parameters](https://github.com/ruchikaverma-iitg/ML-DL-RL_Codes/blob/master/Hands%20on%20Deep%20Learning/Deep%20Neural%20Networks%20with%20PyTorch/L9_training_slope_and_bias.ipynb)| 1D Linear regression model using 2 parameters (w and b). Visualize the data space and the parameter space during training via batch gradient descent|
|10|[Stochastic_gradient_descent](https://github.com/ruchikaverma-iitg/ML-DL-RL_Codes/blob/master/Hands%20on%20Deep%20Learning/Deep%20Neural%20Networks%20with%20PyTorch/L10_stochastic_gradient_descent.ipynb)| 1D Linear regression using stochastic gradient descent|
|11|[Mini_batch_gradient_descent](https://github.com/ruchikaverma-iitg/ML-DL-RL_Codes/blob/master/Hands%20on%20Deep%20Learning/Deep%20Neural%20Networks%20with%20PyTorch/L11_mini-batch_gradient_descent.ipynb)| 1D Linear regression using mini-batch gradient descent. This code also includes comparison between batch, stochastic and mini-batch gradient descent with different batch sizes|
|12|[Mini_batch_gradient_descent2](https://github.com/ruchikaverma-iitg/ML-DL-RL_Codes/blob/master/Hands%20on%20Deep%20Learning/Deep%20Neural%20Networks%20with%20PyTorch/L12_PyTorchway.ipynb)| 1D Linear regression using  PyTorch build-in functions|
|13|[Models_with_different_LR](https://github.com/ruchikaverma-iitg/ML-DL-RL_Codes/blob/master/Hands%20on%20Deep%20Learning/Deep%20Neural%20Networks%20with%20PyTorch/L13_Models_with_different_LR.ipynb)| 1D Linear regression with different learning rates and view results such as training and validation losses at different LR|
|14|[Multiple_linear_regression_prediction](https://github.com/ruchikaverma-iitg/ML-DL-RL_Codes/blob/master/Hands%20on%20Deep%20Learning/Deep%20Neural%20Networks%20with%20PyTorch/L14_multiple_linear_regression_prediction.ipynb)| Multiple linear regression prediction (preparing forward propagation with 1xn tensor input)|
|15|[Multiple_linear_regression_training](https://github.com/ruchikaverma-iitg/ML-DL-RL_Codes/blob/master/Hands%20on%20Deep%20Learning/Deep%20Neural%20Networks%20with%20PyTorch/L15_multiple_linear_regression_training.ipynb)| Multiple linear regression training with input of 1xn tensor|
|16|[Multi_target_linear_regression](https://github.com/ruchikaverma-iitg/ML-DL-RL_Codes/blob/master/Hands%20on%20Deep%20Learning/Deep%20Neural%20Networks%20with%20PyTorch/L16_multi-target_linear_regression.ipynb)| Multiple target linear regression prediction (forward propagation)|
|17|[training_multiple_output_linear_regression](https://github.com/ruchikaverma-iitg/ML-DL-RL_Codes/blob/master/Hands%20on%20Deep%20Learning/Deep%20Neural%20Networks%20with%20PyTorch/L17_training_multiple_output_linear_regression.ipynb)| pytorch build in functions to train multiple target linear regression|
|18|[logistic_regression_prediction](https://github.com/ruchikaverma-iitg/ML-DL-RL_Codes/blob/master/Hands%20on%20Deep%20Learning/Deep%20Neural%20Networks%20with%20PyTorch/L18_logistic_regression_prediction.ipynb)| Prediction using sigmoid/logistic function|
|19|[logistic_regression_prediction](https://github.com/ruchikaverma-iitg/ML-DL-RL_Codes/blob/master/Hands%20on%20Deep%20Learning/Deep%20Neural%20Networks%20with%20PyTorch/L19_Bad_initialization_logistic_regression_with_mean_square_error.ipynb)| Illustration of poor performance of logistic regression via bad parameters initialization|
|20|[Softmax_in_1D](https://github.com/ruchikaverma-iitg/ML-DL-RL_Codes/blob/master/Hands%20on%20Deep%20Learning/Deep%20Neural%20Networks%20with%20PyTorch/L20_softmax_in_1D.ipynb)| Building a Softmax classifier in 1D|
|21|[predicting_MNIST_using_Softmax](https://github.com/ruchikaverma-iitg/ML-DL-RL_Codes/blob/master/Hands%20on%20Deep%20Learning/Deep%20Neural%20Networks%20with%20PyTorch/L21_predicting_MNIST_using_Softmax.ipynb)| Classify handwritten digits from the MNIST database by using Softmax classifier and visualize parameters learned for each class following model training|
|22|[simpleNN_1hiddenlayer](https://github.com/ruchikaverma-iitg/ML-DL-RL_Codes/blob/master/Hands%20on%20Deep%20Learning/Deep%20Neural%20Networks%20with%20PyTorch/L22_simpleNN_1hiddenlayer.ipynb)| Simple Neural Network with 1 hidden layer|
|23|[NN_more_hidden_neurons](https://github.com/ruchikaverma-iitg/ML-DL-RL_Codes/blob/master/Hands%20on%20Deep%20Learning/Deep%20Neural%20Networks%20with%20PyTorch/L23_NN_more_hidden_neurons.ipynb)| Neural Networks with 1 hidden layer (more neurons)|
|24|[Neural_networks](https://github.com/ruchikaverma-iitg/ML-DL-RL_Codes/blob/master/Hands%20on%20Deep%20Learning/Deep%20Neural%20Networks%20with%20PyTorch/L24_Neural_network.ipynb)| Building a neural network with 1 hidden layer to classify noisy XOR data|
|25|[1_layer_neural_network_MNIST](https://github.com/ruchikaverma-iitg/ML-DL-RL_Codes/blob/master/Hands%20on%20Deep%20Learning/Deep%20Neural%20Networks%20with%20PyTorch/L25_1layer_neural_network_MNIST.ipynb)| Neural networks with 1 hidden layer to classify MNIST data|
|26|[Activation_function](https://github.com/ruchikaverma-iitg/ML-DL-RL_Codes/blob/master/Hands%20on%20Deep%20Learning/Deep%20Neural%20Networks%20with%20PyTorch/L26_activation_function.ipynb)| How to apply different Activation functions in Neural Network|
|27|[Different_activations_on_neural_network](https://github.com/ruchikaverma-iitg/ML-DL-RL_Codes/blob/master/Hands%20on%20Deep%20Learning/Deep%20Neural%20Networks%20with%20PyTorch/L27_different_activations_on_neural_network.ipynb)| Apply different activation functions in Neural Network on the MNIST dataset|
|28|[Deep_Neural_Networks](https://github.com/ruchikaverma-iitg/ML-DL-RL_Codes/blob/master/Hands%20on%20Deep%20Learning/Deep%20Neural%20Networks%20with%20PyTorch/L28_Deep_Neural_Networks.ipynb)| Deep Neural Networks with 2 hidden layers on the MNIST dataset|
|29|[Deeper_Neural_Networks](https://github.com/ruchikaverma-iitg/ML-DL-RL_Codes/blob/master/Hands%20on%20Deep%20Learning/Deep%20Neural%20Networks%20with%20PyTorch/L29_Deeper_Neural_Networks.ipynb)| Deep Neural Networks with 3 hidden layers using nn.ModuleList()|
|30|[Dropout_prediction](https://github.com/ruchikaverma-iitg/ML-DL-RL_Codes/blob/master/Hands%20on%20Deep%20Learning/Deep%20Neural%20Networks%20with%20PyTorch/L30_dropout_prediction.ipynb)| Deep Neural Networks with dropout for classification|
|31|[Dropout_regression](https://github.com/ruchikaverma-iitg/ML-DL-RL_Codes/blob/master/Hands%20on%20Deep%20Learning/Deep%20Neural%20Networks%20with%20PyTorch/L31_dropout_Regression.ipynb)| Using dropout in regression|
|32|[Weight_initialization](https://github.com/ruchikaverma-iitg/ML-DL-RL_Codes/blob/master/Hands%20on%20Deep%20Learning/Deep%20Neural%20Networks%20with%20PyTorch/L32_weight_initialization.ipynb)| Performance of neural networks with constant (w = 1) vs. default weight initialization|
|33|[Xavier_initialization](https://github.com/ruchikaverma-iitg/ML-DL-RL_Codes/blob/master/Hands%20on%20Deep%20Learning/Deep%20Neural%20Networks%20with%20PyTorch/L33_Xavier_initialization.ipynb)| Performance of neural networks with uniform, default, and Xavier initialization|
|34|[He_initialization](https://github.com/ruchikaverma-iitg/ML-DL-RL_Codes/blob/master/Hands%20on%20Deep%20Learning/Deep%20Neural%20Networks%20with%20PyTorch/L34_He_Initialization.ipynb)| Performance of neural networks with uniform, default, and He initialization|
|35|[MomentumwithPolynomialFunctions](https://github.com/ruchikaverma-iitg/ML-DL-RL_Codes/blob/master/Hands%20on%20Deep%20Learning/Deep%20Neural%20Networks%20with%20PyTorch/L35_MomentumwithPolynomialFunctions.ipynb)| Use of momentum in the model optimization|
|36|[MomentumwithPolynomialFunctions](https://github.com/ruchikaverma-iitg/ML-DL-RL_Codes/blob/master/Hands%20on%20Deep%20Learning/Deep%20Neural%20Networks%20with%20PyTorch/L36_NeuralNetworkswithMomentum.ipynb)| Neural networks model optimization with different momentum values|
|37|[BachNorm](https://github.com/ruchikaverma-iitg/ML-DL-RL_Codes/blob/master/Hands%20on%20Deep%20Learning/Deep%20Neural%20Networks%20with%20PyTorch/L37_BachNorm.ipynb)| Comparison of neural networks with and without batch normalization|
|38|[Convolution](https://github.com/ruchikaverma-iitg/ML-DL-RL_Codes/blob/master/Hands%20on%20Deep%20Learning/Deep%20Neural%20Networks%20with%20PyTorch/L38_Convolution.ipynb)| Convolution on an image and estimate the output size using kernel of K size|
|39|[MaxPooling](https://github.com/ruchikaverma-iitg/ML-DL-RL_Codes/blob/master/Hands%20on%20Deep%20Learning/Deep%20Neural%20Networks%20with%20PyTorch/L39_Activation_max_pooling.ipynb)| Application of activation function and max pooling|
|40|[Multiple_Channel_Convolution](https://github.com/ruchikaverma-iitg/ML-DL-RL_Codes/blob/master/Hands%20on%20Deep%20Learning/Deep%20Neural%20Networks%20with%20PyTorch/L40_Multiple%20Channel%20Convolution.ipynb)| Convolutions using multiple input and output channels|
|41|[ConvolutionalNeuralNetworkexample](https://github.com/ruchikaverma-iitg/ML-DL-RL_Codes/blob/master/Hands%20on%20Deep%20Learning/Deep%20Neural%20Networks%20with%20PyTorch/L41_ConvolutionalNeuralNetworkexample.ipynb)| Example of convolutional neural network|
|42|[CNN_Small_Image](https://github.com/ruchikaverma-iitg/ML-DL-RL_Codes/blob/master/Hands%20on%20Deep%20Learning/Deep%20Neural%20Networks%20with%20PyTorch/L42_CNN_Small_Image.ipynb)| CNN on a small MNIST image, visualize parameters and plot testing image activations after each layer. The code also plot the mis-classified samples|
